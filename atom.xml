<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <id>https://huran111.github.io</id>
    <title>胡的博客</title>
    <updated>2020-02-21T03:34:59.507Z</updated>
    <generator>https://github.com/jpmonette/feed</generator>
    <link rel="alternate" href="https://huran111.github.io"/>
    <link rel="self" href="https://huran111.github.io/atom.xml"/>
    <subtitle>相信代码改变世界,公众号请关注&lt;小胡的博客&gt; CSND地址 : https://blog.csdn.net/qq_31493829</subtitle>
    <logo>https://huran111.github.io/images/avatar.png</logo>
    <icon>https://huran111.github.io/favicon.ico</icon>
    <rights>All rights reserved 2020, 胡的博客</rights>
    <entry>
        <title type="html"><![CDATA[GO语言-01]]></title>
        <id>https://huran111.github.io/post/go-yu-yan-01</id>
        <link href="https://huran111.github.io/post/go-yu-yan-01">
        </link>
        <updated>2020-01-16T06:18:35.000Z</updated>
        <summary type="html"><![CDATA[<p><img src="https://huran111.github.io/post-images/1579155552763.png" alt="">
<img src="https://huran111.github.io/post-images/1579155560903.png" alt="">
<img src="https://huran111.github.io/post-images/1579155674349.png" alt=""></p>
<h3 id="关键字">关键字</h3>
<p>Go语言中有25个关键字：
<img src="https://huran111.github.io/post-images/1579155751251.png" alt="">
还有37个保留字
<img src="https://huran111.github.io/post-images/1579155779570.png" alt=""></p>
]]></summary>
        <content type="html"><![CDATA[<p><img src="https://huran111.github.io/post-images/1579155552763.png" alt="">
<img src="https://huran111.github.io/post-images/1579155560903.png" alt="">
<img src="https://huran111.github.io/post-images/1579155674349.png" alt=""></p>
<h3 id="关键字">关键字</h3>
<p>Go语言中有25个关键字：
<img src="https://huran111.github.io/post-images/1579155751251.png" alt="">
还有37个保留字
<img src="https://huran111.github.io/post-images/1579155779570.png" alt=""></p>
<!-- more -->
<p>撒地方撒地方是是</p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[RabbitMq]]></title>
        <id>https://huran111.github.io/post/rabbitmq</id>
        <link href="https://huran111.github.io/post/rabbitmq">
        </link>
        <updated>2019-12-30T01:21:06.000Z</updated>
        <content type="html"><![CDATA[<p>AMQP协议：一个提供统一消息服务的应用层标准高级消息队列协议
<img src="https://huran111.github.io/post-images/1577669323241.png" alt="">
<img src="https://huran111.github.io/post-images/1577669330027.png" alt="">
<img src="https://huran111.github.io/post-images/1577669344711.png" alt="">
<img src="https://huran111.github.io/post-images/1577669354435.png" alt="">
<img src="https://huran111.github.io/post-images/1577669361874.png" alt="">
<img src="https://huran111.github.io/post-images/1577669368546.png" alt="">
<img src="https://huran111.github.io/post-images/1577669381898.png" alt="">
<img src="https://huran111.github.io/post-images/1577669389078.png" alt="">
<img src="https://huran111.github.io/post-images/1577669395315.png" alt=""></p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[SparkSQL]]></title>
        <id>https://huran111.github.io/post/sparksql</id>
        <link href="https://huran111.github.io/post/sparksql">
        </link>
        <updated>2019-11-14T06:44:47.000Z</updated>
        <content type="html"><![CDATA[<h3 id="sparksql">SparkSql</h3>
<ol>
<li>是spark上的高级模块，SparkSql是一个SQL解析引擎，将SQL解析成特殊的RDD(DataFrame)，然后在Spark集群中运行</li>
<li>SparkSQL是用来处理结构化数据的</li>
<li>SparkSQL支持两种编程API   sql方式   DataFrame的方式</li>
<li>SparkSQL兼容hive，元数据库，SQL语法，udf,序列化，反序列化</li>
<li>SpqrkSql支持统一的数据源，提供标准连接</li>
</ol>
<h3 id="datafrage">DataFrage</h3>
<p><img src="https://huran111.github.io/post-images/1573715841094.png" alt=""></p>
<h3 id="rdd和dataframe的区别">RDD和DataFrame的区别</h3>
<ol>
<li>DataFrame里面存放的结构化数据的描述信息，DataFrame要有表头（表的描述信息）,描述了有多少列，类型的信息</li>
<li>DataFrame是特殊的RDD(RDD+Schema信息就变成了DataFrame)
<img src="https://huran111.github.io/post-images/1573716239364.png" alt=""></li>
</ol>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[Spark执行过程]]></title>
        <id>https://huran111.github.io/post/spark-zhi-xing-guo-cheng</id>
        <link href="https://huran111.github.io/post/spark-zhi-xing-guo-cheng">
        </link>
        <updated>2019-11-14T01:46:16.000Z</updated>
        <content type="html"><![CDATA[<p><img src="https://huran111.github.io/post-images/1573695989442.png" alt=""></p>
<p>四个过程：
1 构建RDD(driver端)
2 切分stage（Driver端）
3 调度task(Driver端)
4 执行task（Executor）
1.构建DAG 有向无环图（RDD方法）
2.DAGScheduler 切分 stage中生成的task以taskset的形式给TaskScheduler（切分的依据是shuffle）
3.TaskScheduler  task调度到executor里面
4.Worker 执行线程  丢入线程池中</p>
<p>DAG:有像无环图
功能：描述了 多个RDD的转换过程任务执行的时候可以按照DAG的描述执行真正的计算
DAG是有边界的：开始 通过SparkContext创建RDD 结束 触发action 调用run  job就是一个完整的DAG就形成了</p>
<p>一个RDD只是描述了 数据计算过程中的一个环节</p>
<p>一个DAG中可能产生多种不同类型和功能的task 会有不同的阶段</p>
<p>DAGScheduler:将一个DAG切分成一到多个Stage,DAGScheduler切分的依据是Shuffle（宽依赖）</p>
<p>为什么要切分 stage?
一个复杂的业务逻辑 ，将多台机器具有相同属性的数据聚合到一台机器上
如果有shuffle 意味着前面阶段产生的结果后，才能执行下一个阶段。
在同一个stage中，会有多个算子，我们称其为pipeline(流水线：严格按照流程，顺序执行)</p>
<p>RDD和它依赖的父RDD的关系又两种不同的类型：1 窄依赖 2 宽依赖</p>
<p>shuffle重要的依据，父RDD的一个分区的数据，给子RDD的多个分区</p>
<p>shuffle：父RDD一个分区中的数据如果给了子RDD的多个分区
shuffle：会有网络传输数据，但是有网络传输数据，不一定就是shuffle</p>
<p>DateSet 是spark1.6以后推出新的api,也是一个分布式数据集，于RDD相比，保存了很多的描述信息，概念上等同于数据库中的二维表。spark在运行时可以被优化
DateSet里面对应的数据是强类型的。
DateSet特点：
1 一系列分区
2 依赖关系
3 每个切片会有对应的函数
4  kv类型shuffle
5 优化执行计划</p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[scala学习]]></title>
        <id>https://huran111.github.io/post/scala-xue-xi</id>
        <link href="https://huran111.github.io/post/scala-xue-xi">
        </link>
        <updated>2019-11-01T06:20:58.000Z</updated>
        <content type="html"><![CDATA[<p><img src="https://huran111.github.io/post-images/1572589264856.png" alt=""></p>
<ol>
<li>val f1=(name:String)=&gt;println(name）
<img src="https://huran111.github.io/post-images/1572590165231.png" alt=""></li>
</ol>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[SPARK RDD]]></title>
        <id>https://huran111.github.io/post/spark-rdd</id>
        <link href="https://huran111.github.io/post/spark-rdd">
        </link>
        <updated>2019-11-01T01:10:35.000Z</updated>
        <content type="html"><![CDATA[<ol>
<li>RDD是一个基本的抽象，操作RDD就像操作一个本地集合一样，降低了编程的复杂度</li>
<li>RDD的算子分为两类，一类是TRansformation（lazy） 一类是Action（触发任务执行）</li>
<li>创建RDD的几种方式？
1.通过外部的存储系统创建RDD(parallelize函数和makeRDD函数(只能在driver端))
2.将Driver的Scala集合通过冰箱华的方式编程RDD(试验，测验)
3.调用一个已经存在的RDD的TRansformation，会生成一个新的RDD</li>
</ol>
<h3 id="rdd算子">RDD算子</h3>
<p>val rdd2=sc.parallelize(List(5,6,4,7,3,8,3,4,23)).map(_*2).sortBy(x=&gt;x,true)
rdd2.collect
<img src="https://huran111.github.io/post-images/1572571797119.png" alt="">
<img src="https://huran111.github.io/post-images/1572571921917.png" alt="">
并集
<img src="https://huran111.github.io/post-images/1572572156858.png" alt="">
<img src="https://huran111.github.io/post-images/1572572368905.png" alt="">
<img src="https://huran111.github.io/post-images/1572572883250.png" alt="">
<img src="https://huran111.github.io/post-images/1572573175761.png" alt="">
<img src="https://huran111.github.io/post-images/1572573550433.png" alt=""></p>
<h3 id="rdd分区的数据取决于哪些因素">RDD分区的数据取决于哪些因素？</h3>
<ol>
<li>如果是将Driver端的Scala集合并行化创建RDD,并且没有指定RDD的分区，RDD的分区就是为该app分配中的核数</li>
<li>如果是从hdfs中读取数据创建RDD，并且设置了最新的分区数量是1，那么RDD的分区数据即使输入切片的数据，如果不设置最小的分区数量，即spark调用textfile方法会默认传入2，那么RDD的分区数量会大于等于输入的切片的数量</li>
</ol>
<h3 id="rdd的map方法真正在executor中执行时是一条一条的将数据拿出来处理">RDD的map方法，真正在Executor中执行时，是一条一条的将数据拿出来处理</h3>
<p>mapPartitionsWithIndex 一次拿出一个分区（分区中没有数据，记录要读取有多少条数据）并且可以将分区的编号取出来。
功能：取分区中对应的数据时还可以将分区的编号取出来，这样就可以知道这个数据时哪个分区的</p>
<ul>
<li>一个分区对应一个Task</li>
<li><img src="https://huran111.github.io/post-images/1572599528370.png" alt=""></li>
<li></li>
</ul>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[月度交流笔记]]></title>
        <id>https://huran111.github.io/post/yue-du-jiao-liu-bi-ji</id>
        <link href="https://huran111.github.io/post/yue-du-jiao-liu-bi-ji">
        </link>
        <updated>2019-10-29T01:13:16.000Z</updated>
        <content type="html"><![CDATA[<h3 id="启动spark-shell">启动spark shell</h3>
<pre><code>./spark-shell  --master spark://hadoop:7077 --executor-memory 1g --total-executor-cores 12 
</code></pre>
<h3 id="运行demo">运行demo</h3>
<pre><code>bin/spark-submit  --master  spark://hadoop:7077 --class org.apache.spark.examples.SparkPi --executor-memory 2048mb --total-executor-cores 1  examples/jars/spark-examples_2.11-2.4.4.jar  50
</code></pre>
<h3 id="将数据导入hive">将数据导入hive</h3>
<pre><code> sqoop import --connect jdbc:mysql://192.168.10.30:3306/test --username root --password root --table  ins_entry_201905 --hive-import --hive-table ins_entry_201905-m 3
</code></pre>
<h3 id="保存到hdfs">保存到hdfs</h3>
<pre><code>insert overwrite directory &quot;/cheliang/20191029&quot; row format delimited fields terminated by ',' select  count(1),vehicle_class  from ins_entry_201905 group by vehicle_class;
</code></pre>
<h3 id="将结果保存到mysql">将结果保存到mysql</h3>
<p>sqoop export  <br>
--connect &quot;jdbc:mysql://192.168.10.30:3306/test?useUnicode=true&amp;characterEncoding=utf-8&quot;<br>
--username root <br>
--password root <br>
--table class_count <br>
--m 1 <br>
--export-dir /cheliang/20191029/ <br>
--input-fields-terminated-by ','<br>
--columns=&quot;count,class&quot;</p>
<h3 id="sqoop导入结果到hdfs">sqoop导入结果到hdfs</h3>
<p>sqoop import --connect jdbc:mysql://192.168.10.30:3306/test --username root --password root --table ins_entry_201905 --target-dir /cheliang/20191030</p>
<h3 id="启动spark-sql">启动spark sql</h3>
<p>./spark-sql  --master spark://hadoop:7077</p>
<ol>
<li>在sparkSQL中执行
create table users(id bigint,name string) row format delimited fields terminated by ',';</li>
<li>load data local inpath '/root/user.txt' into table users;
<img src="https://huran111.github.io/post-images/1572317275723.png" alt=""></li>
</ol>
<p>整合hive:
<img src="https://huran111.github.io/post-images/1572317707192.png" alt="">
<img src="https://huran111.github.io/post-images/1572317626408.png" alt=""></p>
<p>spark中执行例子：
./spark-submit --master spark://hadoop:7077 --class spark.HiveOnSpark    --executor-memory 2048mb --total-executor-cores 2 /opt/module/spark/dfsj-spark-1.0-SNAPSHOT-shaded.jar</p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[kafka入门介绍]]></title>
        <id>https://huran111.github.io/post/kafka-ru-men-jie-shao</id>
        <link href="https://huran111.github.io/post/kafka-ru-men-jie-shao">
        </link>
        <updated>2019-09-29T08:39:01.000Z</updated>
        <content type="html"><![CDATA[<h3 id="kafka作为一个分布式的流平台这到底意味着什么">Kafka作为一个分布式的流平台，这到底意味着什么？</h3>
<p>我们认为，一个流处理平台具有三个关键能力：</p>
<p>发布和订阅消息(流)，在这方面，它类似于一个消息队列或企业消息系统。
以容错(故障转移)的方式存储消息(流)。
在消息流发生时处理它们。
什么是kafka的优势？它主要应用于2大类应用：
构建实时的流数据管道，可靠地获取系统和应用程序之间的数据。
构建实时流的应用程序，对数据流进行转换或反应。
要了解kafka是如何做这些事情的，让我们从下到上深入探讨kafka的能力。</p>
<p>首先几个概念：
kafka作为一个集群运行在一个或多个服务器上。
kafka集群存储的消息是以topic为类别记录的。
每个消息（也叫记录record，我习惯叫消息）是由一个key，一个value和时间戳构成。
kafka有四个核心API：
应用程序使用 Producer API 发布消息到1个或多个topic（主题）。
应用程序使用 Consumer API 来订阅一个或多个topic，并处理产生的消息。
应用程序使用 Streams API 充当一个流处理器，从1个或多个topic消费输入流，并生产一个输出流到1个或多个输出topic，有效地将输入流转换到输出流。
Connector API允许构建或运行可重复使用的生产者或消费者，将topic连接到现有的应用程序或数据系统。例如，一个关系数据库的连接器可捕获每一个变化。
<img src="https://huran111.github.io/post-images/1569746404947.png" alt=""></p>
<h3 id="首先来了解一下kafka所使用的基本术语">首先来了解一下Kafka所使用的基本术语：</h3>
<p>Topic
Kafka将消息种子(Feed)分门别类，每一类的消息称之为一个主题(Topic).</p>
<p>Producer
发布消息的对象称之为主题生产者(Kafka topic producer)</p>
<p>Consumer
订阅消息并处理发布的消息的种子的对象称之为主题消费者(consumers)</p>
<p>Broker
已发布的消息保存在一组服务器中，称之为Kafka集群。集群中的每一个服务器都是一个代理(Broker). 消费者可以订阅一个或多个主题（topic），并从Broker拉数据，从而消费这些已发布的消息。</p>
<h3 id="主题和日志-topic和log">主题和日志 (Topic和Log)</h3>
<p><img src="https://huran111.github.io/post-images/1569746451359.png" alt=""></p>
<p>每一个分区都是一个顺序的、不可变的消息队列， 并且可以持续的添加。分区中的消息都被分了一个序列号，称之为偏移量(offset)，在每个分区中此偏移量都是唯一的。</p>
<p>Kafka集群保持所有的消息，直到它们过期， 无论消息是否被消费了。 实际上消费者所持有的仅有的元数据就是这个偏移量，也就是消费者在这个log中的位置。 这个偏移量由消费者控制：正常情况当消费者消费消息的时候，偏移量也线性的的增加。但是实际偏移量由消费者控制，消费者可以将偏移量重置为更老的一个偏移量，重新读取消息。 可以看到这种设计对消费者来说操作自如， 一个消费者的操作不会影响其它消费者对此log的处理。 再说说分区。Kafka中采用分区的设计有几个目的。一是可以处理更多的消息，不受单台服务器的限制。Topic拥有多个分区意味着它可以不受限的处理更多的数据。第二，分区可以作为并行处理的单元，稍后会谈到这一点。
<img src="https://huran111.github.io/post-images/1569746475414.png" alt=""></p>
<h3 id="kafka作为一个存储系统">kafka作为一个存储系统</h3>
<p>所有发布消息到消息队列和消费分离的系统，实际上都充当了一个存储系统（发布的消息先存储起来）。Kafka比别的系统的优势是它是一个非常高性能的存储系统。</p>
<p>写入到kafka的数据将写到磁盘并复制到集群中保证容错性。并允许生产者等待消息应答，直到消息完全写入。</p>
<p>kafka的磁盘结构 - 无论你服务器上有50KB或50TB，执行是相同的。</p>
<p>client来控制读取数据的位置。你还可以认为kafka是一种专用于高性能，低延迟，提交日志存储，复制，和传播特殊用途的分布式文件系统。</p>
<h3 id="kafka的流处理">kafka的流处理</h3>
<p>仅仅读，写和存储是不够的，kafka的目标是实时的流处理。</p>
<p>在kafka中，流处理持续获取输入topic的数据，进行处理加工，然后写入输出topic。例如，一个零售APP，接收销售和出货的输入流，统计数量或调整价格后输出。</p>
<p>可以直接使用producer和consumer API进行简单的处理。对于复杂的转换，Kafka提供了更强大的Streams API。可构建聚合计算或连接流到一起的复杂应用程序。</p>
<p>助于解决此类应用面临的硬性问题：处理无序的数据，代码更改的再处理，执行状态计算等。</p>
<p>Sterams API在Kafka中的核心：使用producer和consumer API作为输入，利用Kafka做状态存储，使用相同的组机制在stream处理器实例之间进行容错保障。</p>
<p>消息传递，存储和流处理的组合看似反常，但对于Kafka作为流式处理平台的作用至关重要。</p>
<p>像HDFS这样的分布式文件系统允许存储静态文件来进行批处理。这样系统可以有效地存储和处理来自过去的历史数据。</p>
<p>传统企业的消息系统允许在你订阅之后处理未来的消息：在未来数据到达时处理它。</p>
<p>Kafka结合了这两种能力，这种组合对于kafka作为流处理应用和流数据管道平台是至关重要的。</p>
<p>批处理以及消息驱动应用程序的流处理的概念：通过组合存储和低延迟订阅，流处理应用可以用相同的方式对待过去和未来的数据。它是一个单一的应用程序，它可以处理历史的存储数据，当它处理到最后一个消息时，它进入等待未来的数据到达，而不是结束。</p>
<p>同样，对于流数据管道（pipeline），订阅实时事件的组合使得可以将Kafka用于非常低延迟的管道；但是，可靠地存储数据的能力使得它可以将其用于必须保证传递的关键数据，或与仅定期加载数据或长时间维护的离线系统集成在一起。流处理可以在数据到达时转换它。
<img src="https://huran111.github.io/post-images/1569746708640.png" alt="">
<img src="https://huran111.github.io/post-images/1569746724920.png" alt=""></p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[Flume + Kafka + Spark Streaming整合]]></title>
        <id>https://huran111.github.io/post/flume-kafka-spark-streaming-zheng-he</id>
        <link href="https://huran111.github.io/post/flume-kafka-spark-streaming-zheng-he">
        </link>
        <updated>2019-09-29T04:39:40.000Z</updated>
        <content type="html"><![CDATA[<p><strong>1. 启动kafka,创建一个主题</strong>
./kafka-topics.sh –create –zookeeper hadoop:2181 –replication-factor 1 –partitions 1 –topic flume-kafka-streaming-topic</p>
<p><strong>2.删除主题：</strong>
/bin/kafka-topics –delete –topic test –zookeeper localhost:2181</p>
<p><strong>3.配置Flume文件：</strong>
agent1.sources=avro-source
agent1.channels=logger-channel
agent1.sinks=kafka-sink</p>
<p>#define source
agent1.sources.avro-source.type=avro
agent1.sources.avro-source.bind=0.0.0.0
agent1.sources.avro-source.port=41414</p>
<p>#define channel
agent1.channels.logger-channel.type=memory</p>
<p>#define sink
agent1.sinks.kafka-sink.type=org.apache.flume.sink.kafka.KafkaSink
agent1.sinks.kafka-sink.topic = flume-kafka-streaming-topic
agent1.sinks.kafka-sink.brokerList = hadoop:9092
agent1.sinks.kafka-sink.requiredAcks = 1
agent1.sinks.kafka-sink.batchSize = 20</p>
<p>agent1.sources.avro-source.channels=logger-channel
agent1.sinks.kafka-sink.channel=logger-channel</p>
<p><strong>启动flume-ng</strong></p>
<p>flume-ng agent <br>
--conf $FLUME_HOME/conf <br>
--conf-file $FLUME_HOME/conf/streaming.conf <br>
--name agent1 <br>
-Dflume.root.logger=INFO,console</p>
<p><strong>启动日志生产程序</strong>
bin/kafka-console-producer.sh –broker-list  hadoop:9092  –topic lume-kafka-streaming-topic</p>
<p>bin/kafka-console-consumer.sh –bootstrap-server hadoop02:9092 –topic flume-kafka-streaming-topic –from-beginning</p>
<p><strong>测试Flume</strong></p>
<p>bin/flume-ng avro-client -c conf -H 192.168.10.179 -p 41414 -F /app/log/test.log</p>
<p><strong>运行本地spark程序：</strong></p>
<p>def main(args: Array[String]): Unit = {
if (args.length &lt; 4) {
//Edit Configuration : 192.168.10.179:2181 test flume-kafka-streaming-topic 1
System.err.println(&quot;Usage: FlumeKafkaReceiverWordCount <zkQuorum> <group> <topics> <numThreads>&quot;)
System.exit(1)
}</p>
<p>val Array(zkQuorum, group, topics, numThreads) = args</p>
<p>val sparkConf = new SparkConf().setAppName(&quot;FlumeKafkaReceiverWordCount&quot;).setMaster(&quot;local[2]&quot;)
//val sparkConf = new SparkConf()</p>
<p>val ssc = new StreamingContext(sparkConf, Seconds(5))</p>
<p>val topicMap = topics.split(&quot;,&quot;).map((_, numThreads.toInt)).toMap</p>
<p>val messages = KafkaUtils.createStream(ssc, zkQuorum, group, topicMap)</p>
<p>messages.map(<em>.<em>2).flatMap(</em>.split(&quot; &quot;)).map((</em>, 1)).reduceByKey(_ + _).print()</p>
<p>ssc.start()
ssc.awaitTermination()
}</p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[Spark]]></title>
        <id>https://huran111.github.io/post/spark</id>
        <link href="https://huran111.github.io/post/spark">
        </link>
        <updated>2019-09-29T04:31:43.000Z</updated>
        <content type="html"><![CDATA[<h3 id="spark是什么">Spark是什么</h3>
<pre><code>   Apache Spark 是专为大规模数据处理而设计的快速通用的计算引擎。Spark是UC Berkeley AMP lab (加州大学伯克利分校的AMP实验室)所开源的类Hadoop MapReduce的通用并行框架，Spark，拥有Hadoop MapReduce所具有的优点；但不同于MapReduce的是——Job中间输出结果可以保存在内存中，从而不再需要读写HDFS，因此Spark能更好地适用于数据挖掘与机器学习等需要迭代的MapReduce的算法。
 Spark 是一种与 Hadoop 相似的开源集群计算环境，但是两者之间还存在一些不同之处，这些有用的不同之处使 Spark 在某些工作负载方面表现得更加优越，换句话说，Spark 启用了内存分布数据集，除了能够提供交互式查询外，它还可以优化迭代工作负载。
</code></pre>
<p>Spark 是在 Scala 语言中实现的，它将 Scala 用作其应用程序框架。与 Hadoop 不同，Spark 和 Scala 能够紧密集成，其中的 Scala 可以像操作本地集合对象一样轻松地操作分布式数据集。
尽管创建 Spark 是为了支持分布式数据集上的迭代作业，但是实际上它是对 Hadoop 的补充，可以在 Hadoop 文件系统中并行运行。通过名为 Mesos 的第三方集群框架可以支持此行为。Spark 由加州大学伯克利分校 AMP 实验室 (Algorithms, Machines, and People Lab) 开发，可用来构建大型的、低延迟的数据分析应用程序。</p>
<h3 id="什么是sparksql">什么是SparkSql?</h3>
<p>Spark SQL是Spark用来处理结构化数据的一个模块，它提供了2个编程抽象：DataFrame和DataSet，并且作为分布式SQL查询引擎的作用。</p>
<p>我们已经学习了Hive，它是将Hive SQL转换成MapReduce然后提交到集群上执行，大大简化了编写MapReduc的程序的复杂性，由于MapReduce这种计算模型执行效率比较慢。所有Spark SQL的应运而生，它是将Spark SQL转换成RDD，然后提交到集群执行，执行效率非常快！</p>
<h3 id="sparksql特点">SparkSql特点？</h3>
<ol>
<li>易整合
<img src="https://huran111.github.io/post-images/1569731678073.png" alt="">
2.统一的数据访问方式
<img src="https://huran111.github.io/post-images/1569731706084.png" alt="">
3.兼容Hive
<img src="https://huran111.github.io/post-images/1569731720818.png" alt="">
4.标准的数据连接
<img src="https://huran111.github.io/post-images/1569731734307.png" alt=""></li>
</ol>
<h3 id="什么是dataframe">什么是DataFrame?</h3>
<pre><code> 与RDD类似，DataFrame也是一个分布式数据容器。然而DataFrame更像传统数据库的二维表格，除了数据以外，还记录数据的结构信息，即schema。同时，与Hive类似，DataFrame也支持嵌套数据类型（struct、array和map）。从API易用性的角度上看，DataFrame API提供的是一套高层的关系操作，比函数式的RDD API要更加友好，门槛更低。
	 ![](https://huran111.github.io/post-images/1569731760042.png)
</code></pre>
<p>上图直观地体现了DataFrame和RDD的区别。左侧的RDD[Person]虽然以Person为类型参数，但Spark框架本身不了解Person类的内部结构。而右侧的DataFrame却提供了详细的结构信息，使得Spark SQL可以清楚地知道该数据集中包含哪些列，每列的名称和类型各是什么。DataFrame是为数据提供了Schema的视图。可以把它当做数据库中的一张表来对待，DataFrame也是懒执行的。性能上比RDD要高，主要原因：</p>
<p>优化的执行计划：查询计划通过Spark catalyst optimiser进行优化。</p>
<p><img src="https://huran111.github.io/post-images/1569731782276.png" alt="">
<img src="https://huran111.github.io/post-images/1569731797929.png" alt="">
注意：临时表是Session范围内的，Session退出后，表就失效了。如果想应用范围内有效，可以使用全局表。注意使用全局表时需要全路径访问，如：global_temp.people</p>
<h3 id="dsl风格语法">DSL风格语法</h3>
<p><img src="https://huran111.github.io/post-images/1569731843143.png" alt=""></p>
<p><img src="https://huran111.github.io/post-images/1569731858597.png" alt=""></p>
<h3 id="sparkstreaming是什么">SparkStreaming是什么？</h3>
<pre><code>Spark Streaming用于流式数据的处理。Spark Streaming支持的数据输入源很多，例如：Kafka、Flume、Twitter、ZeroMQ和简单的TCP套接字等等。数据输入后可以用Spark的高度抽象原语如：map、reduce、join、window等进行运算。而结果也能保存在很多地方，如HDFS，数据库等。
</code></pre>
<p><img src="https://huran111.github.io/post-images/1569731912291.png" alt="">
和Spark基于RDD的概念很相似，Spark Streaming使用离散化流(discretized stream)作为抽象表示，叫作DStream。DStream 是随时间推移而收到的数据的序列。在内部，每个时间区间收到的数据都作为 RDD 存在，而DStream是由这些RDD所组成的序列(因此得名“离散化”)。</p>
<h3 id="sparkstreaming特点">SparkStreaming特点</h3>
<p><img src="https://huran111.github.io/post-images/1569731935208.png" alt="">
<img src="https://huran111.github.io/post-images/1569731939464.png" alt="">
<img src="https://huran111.github.io/post-images/1569731943730.png" alt=""></p>
<p><img src="https://huran111.github.io/post-images/1569731953410.png" alt=""></p>
]]></content>
    </entry>
</feed>